# pyqt5与scrapy打包

## 准备工作
1. scrapy脚本启动（在scrapy.Cfg目录下创建一个carwl.py文件）
2. 修改文件：
~~~python
from scrapy.crawler import CrawlerProcess 
from scrapy.utils.project import get_project_settings
def run(Q):
    # from chanel.spiders.chanel_shops import ChanelShopsSpider
    process = CrawlerProcess(get_project_settings())
    # 'followall' is the name of one of the spiders of the project.
    process.crawl('followall')
    # 如果需要添加参数，像这样
    process.crawl('followall',**{'Q',Q})
    # 如果要添加
    process.start()  # the script will block here until the crawling is finished
~~~
3. 启动文件scrapy启动 

4. 导入一些模块 
~~~python
# 必须的
import urllib.robotparser
import scrapy.spiderloader
import scrapy.statscollectors
import scrapy.logformatter
import scrapy.dupefilters
import scrapy.squeues
import scrapy.extensions.spiderstate
import scrapy.extensions.corestats
import scrapy.extensions.telnet
import scrapy.extensions.logstats
import scrapy.extensions.memusage
import scrapy.extensions.memdebug
import scrapy.extensions.feedexport
import scrapy.extensions.closespider
import scrapy.extensions.debug
import scrapy.extensions.httpcache
import scrapy.extensions.statsmailer
import scrapy.extensions.throttle
import scrapy.core.scheduler
import scrapy.core.engine
import scrapy.core.scraper
import scrapy.core.spidermw
import scrapy.core.downloader
import scrapy.downloadermiddlewares.stats
import scrapy.downloadermiddlewares.httpcache
import scrapy.downloadermiddlewares.cookies
import scrapy.downloadermiddlewares.useragent
import scrapy.downloadermiddlewares.httpproxy
import scrapy.downloadermiddlewares.ajaxcrawl
import scrapy.downloadermiddlewares.chunked
import scrapy.downloadermiddlewares.decompression
import scrapy.downloadermiddlewares.defaultheaders
import scrapy.downloadermiddlewares.downloadtimeout
import scrapy.downloadermiddlewares.httpauth
import scrapy.downloadermiddlewares.httpcompression
import scrapy.downloadermiddlewares.redirect
import scrapy.downloadermiddlewares.retry
import scrapy.downloadermiddlewares.robotstxt
import scrapy.spidermiddlewares.depth
import scrapy.spidermiddlewares.httperror
import scrapy.spidermiddlewares.offsite
import scrapy.spidermiddlewares.referer
import scrapy.spidermiddlewares.urllength
import scrapy.pipelines
import scrapy.core.downloader.handlers.http
import scrapy.core.downloader.handlers.datauri
import scrapy.core.downloader.handlers.file
import scrapy.core.downloader.handlers.s3
import scrapy.core.downloader.handlers.ftp
import scrapy.core.downloader.contextfactory
~~~
## 准备pyqt5
1. 修改pyqt5 gui文件 
~~~python
# 导入一些文件
from multiprocessing import Process, Manager, freeze_support

# 还需要你的scrapy启动脚本也导入

~~~
2. 修改启动的按钮功能 
~~~python
# 注意类中的self

from multiprocessing import Process, Manager, freeze_support

# __init__初始化
Q = Manager().Queue()

# 启动;
start = Process(target=run,args=(Q,))
start.start()

# 停止
start.terminate()

# 在程序入口添加：
freeze_support()
~~~
3. 将scrapy中需要print输出的部分换成 
~~~python
# 注意类中的self
Q.put('')

~~~
4. 在scrapy爬虫末尾添加 
~~~
def close(spider, reason):
    spider.Q.put('采集结束')
~~~

5. 在pyqt5中添加这部分内容：
~~~
class LogThread(QThread):
    def __init__(self, gui):
        super(LogThread, self).__init__()
        self.gui = gui

    def run(self) -> None:
        while True:
            if not self.gui.Q.empty():
                self.gui.log_browser.append(self.gui.Q.get())

                cursor = self.gui.log_browser.textCursor()
                pos = len(self.gui.log_browser.toPlainText())
                cursor.setPosition(pos)
                self.gui.log_browser.setTextCursor(cursor)

                if '采集结束' in self.gui.log_browser.toPlainText():
                    self.gui.crawl_button.setText('Start crawl')
                    break

                self.msleep(20)  # 避免内容爆炸，导致崩溃


~~~
6. 将第5步中的内容也利用按钮启动停止 
~~~python
# __init__初始化
log_thread = LogThread(self)

# 启动
log_thread.start()

# 停止
log_thread.terminate()
~~~
# 打包
1. 复制 scapy 安装文件中的VERSION与mime.types到scrapy.cfg目录

2. 当前目录创建一个生成scrapy.cfg的文件 generate_cfg.py
~~~python
# 复制自己的scrapy.cfg文件内容
data = """

"""

with open('scrapy.cfg', 'w') as f:
    f.write(data)
~~~
1. 打包命令（修改自己的项目名与pyqt5文件名，我这是books与gui.py）
~~~
# 命令：
pyinstaller -F -w --add-data=mime.types;scrapy --add-data=VERSION;scrapy --add-data=books/*.py;books --add-data=books/spiders/*.py;books/spiders --runtime-hook=generate_cfg.py gui.py

~~~
